{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://pypi.python.org/packages/7b/98/7445165b69d4e95403372c07845ad1756af509ac85fc33c5b88e1c3f90c9/xlrd-0.9.0.tar.gz#md5=61102459833cc31d6b05404325fa45a8\n",
      "  Looking up \"https://pypi.python.org/packages/7b/98/7445165b69d4e95403372c07845ad1756af509ac85fc33c5b88e1c3f90c9/xlrd-0.9.0.tar.gz\" in the cache\n",
      "  Current age based on date: 68\n",
      "  Freshness lifetime from max-age: 31557600\n",
      "  The response is \"fresh\", returning cached response\n",
      "  31557600 > 68\n",
      "  Using cached xlrd-0.9.0.tar.gz\n",
      "  Downloading from URL https://pypi.python.org/packages/7b/98/7445165b69d4e95403372c07845ad1756af509ac85fc33c5b88e1c3f90c9/xlrd-0.9.0.tar.gz#md5=61102459833cc31d6b05404325fa45a8\n",
      "  Running setup.py (path:/tmp/pip-ta3y2ecq-build/setup.py) egg_info for package from https://pypi.python.org/packages/7b/98/7445165b69d4e95403372c07845ad1756af509ac85fc33c5b88e1c3f90c9/xlrd-0.9.0.tar.gz#md5=61102459833cc31d6b05404325fa45a8\n",
      "    Running command python setup.py egg_info\n",
      "    running egg_info\n",
      "    creating pip-egg-info/xlrd.egg-info\n",
      "    writing dependency_links to pip-egg-info/xlrd.egg-info/dependency_links.txt\n",
      "    writing top-level names to pip-egg-info/xlrd.egg-info/top_level.txt\n",
      "    writing pip-egg-info/xlrd.egg-info/PKG-INFO\n",
      "    writing manifest file 'pip-egg-info/xlrd.egg-info/SOURCES.txt'\n",
      "    reading manifest file 'pip-egg-info/xlrd.egg-info/SOURCES.txt'\n",
      "    writing manifest file 'pip-egg-info/xlrd.egg-info/SOURCES.txt'\n",
      "  Source in /tmp/pip-ta3y2ecq-build has version 0.9.0, which satisfies requirement xlrd==0.9.0 from https://pypi.python.org/packages/7b/98/7445165b69d4e95403372c07845ad1756af509ac85fc33c5b88e1c3f90c9/xlrd-0.9.0.tar.gz#md5=61102459833cc31d6b05404325fa45a8\n",
      "Building wheels for collected packages: xlrd\n",
      "  Running setup.py bdist_wheel for xlrd ... \u001b[?25l  Destination directory: /tmp/tmp_cs4pnq5pip-wheel-\n",
      "  Running command /root/miniconda3/envs/carnd-term1/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-ta3y2ecq-build/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d /tmp/tmp_cs4pnq5pip-wheel- --python-tag cp35\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib\n",
      "  creating build/lib/xlrd\n",
      "  copying xlrd/info.py -> build/lib/xlrd\n",
      "  copying xlrd/timemachine.py -> build/lib/xlrd\n",
      "  copying xlrd/formatting.py -> build/lib/xlrd\n",
      "  copying xlrd/sheet.py -> build/lib/xlrd\n",
      "  copying xlrd/xldate.py -> build/lib/xlrd\n",
      "  copying xlrd/__init__.py -> build/lib/xlrd\n",
      "  copying xlrd/biffh.py -> build/lib/xlrd\n",
      "  copying xlrd/formula.py -> build/lib/xlrd\n",
      "  copying xlrd/xlsx.py -> build/lib/xlrd\n",
      "  copying xlrd/compdoc.py -> build/lib/xlrd\n",
      "  copying xlrd/book.py -> build/lib/xlrd\n",
      "  copying xlrd/licences.py -> build/lib/xlrd\n",
      "  creating build/lib/xlrd/doc\n",
      "  copying xlrd/doc/xlrd.html -> build/lib/xlrd/doc\n",
      "  copying xlrd/doc/compdoc.html -> build/lib/xlrd/doc\n",
      "  creating build/lib/xlrd/examples\n",
      "  copying xlrd/examples/namesdemo.xls -> build/lib/xlrd/examples\n",
      "  copying xlrd/examples/xlrdnameAPIdemo.py -> build/lib/xlrd/examples\n",
      "  running build_scripts\n",
      "  creating build/scripts-3.5\n",
      "  copying and adjusting scripts/runxlrd.py -> build/scripts-3.5\n",
      "  changing mode of build/scripts-3.5/runxlrd.py from 644 to 755\n",
      "  installing to build/bdist.linux-x86_64/wheel\n",
      "  running install\n",
      "  running install_lib\n",
      "  creating build/bdist.linux-x86_64\n",
      "  creating build/bdist.linux-x86_64/wheel\n",
      "  creating build/bdist.linux-x86_64/wheel/xlrd\n",
      "  copying build/lib/xlrd/info.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  creating build/bdist.linux-x86_64/wheel/xlrd/doc\n",
      "  copying build/lib/xlrd/doc/xlrd.html -> build/bdist.linux-x86_64/wheel/xlrd/doc\n",
      "  copying build/lib/xlrd/doc/compdoc.html -> build/bdist.linux-x86_64/wheel/xlrd/doc\n",
      "  copying build/lib/xlrd/timemachine.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  copying build/lib/xlrd/formatting.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  copying build/lib/xlrd/sheet.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  copying build/lib/xlrd/xldate.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  copying build/lib/xlrd/__init__.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  copying build/lib/xlrd/biffh.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  copying build/lib/xlrd/formula.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  copying build/lib/xlrd/xlsx.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  copying build/lib/xlrd/compdoc.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  creating build/bdist.linux-x86_64/wheel/xlrd/examples\n",
      "  copying build/lib/xlrd/examples/namesdemo.xls -> build/bdist.linux-x86_64/wheel/xlrd/examples\n",
      "  copying build/lib/xlrd/examples/xlrdnameAPIdemo.py -> build/bdist.linux-x86_64/wheel/xlrd/examples\n",
      "  copying build/lib/xlrd/book.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  copying build/lib/xlrd/licences.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  running install_egg_info\n",
      "  running egg_info\n",
      "  creating xlrd.egg-info\n",
      "  writing top-level names to xlrd.egg-info/top_level.txt\n",
      "  writing xlrd.egg-info/PKG-INFO\n",
      "  writing dependency_links to xlrd.egg-info/dependency_links.txt\n",
      "  writing manifest file 'xlrd.egg-info/SOURCES.txt'\n",
      "  reading manifest file 'xlrd.egg-info/SOURCES.txt'\n",
      "  writing manifest file 'xlrd.egg-info/SOURCES.txt'\n",
      "  Copying xlrd.egg-info to build/bdist.linux-x86_64/wheel/xlrd-0.9.0-py3.5.egg-info\n",
      "  running install_scripts\n",
      "  creating build/bdist.linux-x86_64/wheel/xlrd-0.9.0.data\n",
      "  creating build/bdist.linux-x86_64/wheel/xlrd-0.9.0.data/scripts\n",
      "  copying build/scripts-3.5/runxlrd.py -> build/bdist.linux-x86_64/wheel/xlrd-0.9.0.data/scripts\n",
      "  changing mode of build/bdist.linux-x86_64/wheel/xlrd-0.9.0.data/scripts/runxlrd.py to 755\n",
      "  creating build/bdist.linux-x86_64/wheel/xlrd-0.9.0.dist-info/WHEEL\n",
      "  creating '/tmp/tmp_cs4pnq5pip-wheel-/xlrd-0.9.0-cp35-none-any.whl' and adding '.' to it\n",
      "  adding 'xlrd/__init__.py'\n",
      "  adding 'xlrd/biffh.py'\n",
      "  adding 'xlrd/book.py'\n",
      "  adding 'xlrd/compdoc.py'\n",
      "  adding 'xlrd/formatting.py'\n",
      "  adding 'xlrd/formula.py'\n",
      "  adding 'xlrd/info.py'\n",
      "  adding 'xlrd/licences.py'\n",
      "  adding 'xlrd/sheet.py'\n",
      "  adding 'xlrd/timemachine.py'\n",
      "  adding 'xlrd/xldate.py'\n",
      "  adding 'xlrd/xlsx.py'\n",
      "  adding 'xlrd/doc/compdoc.html'\n",
      "  adding 'xlrd/doc/xlrd.html'\n",
      "  adding 'xlrd/examples/namesdemo.xls'\n",
      "  adding 'xlrd/examples/xlrdnameAPIdemo.py'\n",
      "  adding 'xlrd-0.9.0.data/scripts/runxlrd.py'\n",
      "  adding 'xlrd-0.9.0.dist-info/DESCRIPTION.rst'\n",
      "  adding 'xlrd-0.9.0.dist-info/metadata.json'\n",
      "  adding 'xlrd-0.9.0.dist-info/top_level.txt'\n",
      "  adding 'xlrd-0.9.0.dist-info/WHEEL'\n",
      "  adding 'xlrd-0.9.0.dist-info/METADATA'\n",
      "  adding 'xlrd-0.9.0.dist-info/RECORD'\n",
      "done\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/f4/0c/09/39986983b4a793686e129a320cc5c39cf14f6ce6067f961a8d\n",
      "  Removing source in /tmp/pip-ta3y2ecq-build\n",
      "Successfully built xlrd\n",
      "Installing collected packages: xlrd\n",
      "\n",
      "Successfully installed xlrd-0.9.0\n",
      "Cleaning up...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!pip install -Iv https://pypi.python.org/packages/7b/98/7445165b69d4e95403372c07845ad1756af509ac85fc33c5b88e1c3f90c9/xlrd-0.9.0.tar.gz#md5=61102459833cc31d6b05404325fa45a8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/pandas/util/_decorators.py:118: FutureWarning: The `sheetname` keyword is deprecated, use `sheet_name` instead\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "import pandas as pd\n",
    "import os \n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "demographics = pd.read_excel('/src/Data/ANIML/oasis_cross-sectional.xls', sheetname=3) # load data\n",
    "df = demographics.dropna(how='any') # remove NaN values\n",
    "df_columns = list(demographics.columns)\n",
    "X_columns = np.delete(df_columns, [6,7], None) # X matrix won't have MMSE or CDR scores\n",
    "Xdf = df.reindex(columns=X_columns)\n",
    "X = Xdf.values # creating X\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "# Define function to get list of pngs based on slice number\n",
    "pngs_path='/src/Data/ANIML/OASIS_pngs'\n",
    "def getaxialPNG(path):\n",
    "    l = []\n",
    "    axialslice90_files = []\n",
    "    axialslice91_files = []\n",
    "    axialslice92_files = []\n",
    "    axialslice93_files = []\n",
    "    axialslice94_files = []\n",
    "    axialslice95_files = []\n",
    "    axialslice96_files = []\n",
    "    axialslice97_files = []\n",
    "    axialslice98_files = []\n",
    "    axialslice99_files = []\n",
    "    \n",
    "    for root, directories, filenames in os.walk(path):\n",
    "        \n",
    "        for filename in filenames:\n",
    "            if \".90.\" in filename: \n",
    "                axialslice90_files.append(os.path.join(root, filename))\n",
    "            if \".91.\" in filename: \n",
    "                axialslice91_files.append(os.path.join(root, filename))\n",
    "            if \".92.\" in filename: \n",
    "                axialslice92_files.append(os.path.join(root, filename))\n",
    "            if \".93.\" in filename: \n",
    "                axialslice93_files.append(os.path.join(root, filename))\n",
    "            if \".94.\" in filename: \n",
    "                axialslice94_files.append(os.path.join(root, filename))\n",
    "            if \".95.\" in filename: \n",
    "                axialslice95_files.append(os.path.join(root, filename))\n",
    "            if \".96.\" in filename: \n",
    "                axialslice96_files.append(os.path.join(root, filename))\n",
    "            if \".97.\" in filename: \n",
    "                axialslice97_files.append(os.path.join(root, filename))\n",
    "            if \".98.\" in filename: \n",
    "                axialslice98_files.append(os.path.join(root, filename))\n",
    "            if \".99.\" in filename: \n",
    "                axialslice99_files.append(os.path.join(root, filename))\n",
    "\n",
    "    l = list(zip(axialslice90_files, axialslice91_files, axialslice92_files, axialslice93_files, axialslice94_files, axialslice95_files, axialslice96_files, axialslice97_files, axialslice98_files, axialslice99_files))\n",
    "\n",
    "    return ((np.asarray(l)))\n",
    "\n",
    "axial_files0 = getaxialPNG(pngs_path)\n",
    "axial_X_files = np.take(axial_files0, indices=df.index.values, axis=0) # keeps the images with the same index as X matrix\n",
    "axial90loc, axial91loc, axial92loc, axial93loc, axial94loc, axial95loc, axial96loc, axial97loc, axial98loc, axial99loc = zip(*axial_X_files)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "X_id, X_sex, X_handedness, X_age, X_education, X_SES, X_eTIV, X_nWBV, X_ASF = zip(*X) # unzips big X matrix\n",
    "\n",
    "def sex_translator(X_sex):\n",
    "    X_sex_binary = []\n",
    "    X_sex_encoded = []\n",
    "    for x in X_sex:\n",
    "        if x == 'M':\n",
    "            X_sex_binary.append(1)\n",
    "            X_sex_encoded.append([0,1])\n",
    "        else:\n",
    "            X_sex_binary.append(-1)\n",
    "            X_sex_encoded.append([1,0])\n",
    "    \n",
    "    return(zip(X_sex_binary, X_sex_encoded)) # gives us binary and one-hot encoded for sex\n",
    "           \n",
    "def hand_translator(X_handedness):\n",
    "    X_hand_binary = []\n",
    "    X_hand_encoded = []\n",
    "    for x in X_handedness:\n",
    "        if x == 'R':\n",
    "            X_hand_binary.append(1)\n",
    "            X_hand_encoded.append([0,1])\n",
    "        else:\n",
    "            X_hand_binary.append(-1)\n",
    "            X_hand_encoded.append([1,0])\n",
    "    \n",
    "    return(zip(X_hand_binary, X_hand_encoded)) # same as above but for handedness\n",
    "\n",
    "# turns out all the patients are right-handed\n",
    "# maybe we should all just be left-handed so we don't suffer from Alzheimer's\n",
    "# /frequentist sarcasm\n",
    "\n",
    "X_sex_binary, X_sex_encoded = zip(*sex_translator(X_sex)) # unzipping to get our function outputs\n",
    "X_hand_binary, X_hand_encoded = zip(*hand_translator(X_handedness))\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "def prepPNGimgs(array_of_image_paths):\n",
    "    l = []\n",
    "    for img_file in array_of_image_paths: # for each file in the list of images...\n",
    "        img = cv2.imread(\"{}\".format(img_file)) # read the image...\n",
    "        img = cv2.resize(np.array(img), (224,224)) # resize it to 224 by 224 QUICK WHAT'S 224 SQUARED??\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # makes it grayscale\n",
    "        flatten = gray.flatten() # spaghettifies it into 1 by 50176 (which is 224 squared)\n",
    "        l.append(flatten)\n",
    "    \n",
    "    return(np.asarray(l))\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "Y_CDR_columns = [column_name for column_name in df_columns if column_name == 'CDR']\n",
    "Y_CDR_df = df.reindex(columns=Y_CDR_columns)\n",
    "Y_CDR = Y_CDR_df.values\n",
    "\n",
    "Y_MMSE_columns = [column_name for column_name in df_columns if column_name == 'MMSE']\n",
    "Y_MMSE_df = df.reindex(columns=Y_MMSE_columns)\n",
    "Y_MMSE = Y_MMSE_df.values\n",
    "\n",
    "CDR_threshold_0 = 0 # threshold values by CDR scale\n",
    "CDR_threshold_0point5 = 0.5\n",
    "CDR_threshold_1 = 1\n",
    "\n",
    "MMSE_threshold_24 = 24 # threshold values by MMSE scale\n",
    "MMSE_threshold_18 = 18\n",
    "\n",
    "def CDR_probable_AD_thresholder(Y_CDR, threshold_value):\n",
    "    Y_CDR_binary = []\n",
    "    Y_CDR_encoded = []\n",
    "    for y in Y_CDR:\n",
    "        if y > threshold_value:\n",
    "            Y_CDR_binary.append(1)\n",
    "            Y_CDR_encoded.append([0,1])\n",
    "        else:\n",
    "            Y_CDR_binary.append(-1)\n",
    "            Y_CDR_encoded.append([1,0])\n",
    "\n",
    "    return((zip(Y_CDR_binary, Y_CDR_encoded)))\n",
    "\n",
    "def MMSE_probable_Dementia_thresholder(Y_MMSE, threshold_value):\n",
    "    Y_MMSE_binary = []\n",
    "    Y_MMSE_encoded = []\n",
    "    for y in Y_MMSE:\n",
    "        if y < threshold_value:\n",
    "            Y_MMSE_binary.append(1)\n",
    "            Y_MMSE_encoded.append([0,1])\n",
    "        else:\n",
    "            Y_MMSE_binary.append(-1)\n",
    "            Y_MMSE_encoded.append([1,0])\n",
    "        \n",
    "    return(zip(Y_MMSE_binary, Y_MMSE_encoded))\n",
    "\n",
    "Y_CDR_binary, Y_CDR_encoded = zip(*CDR_probable_AD_thresholder(Y_CDR, CDR_threshold_0))\n",
    "Y_MMSE_binary, Y_MMSE_encoded = zip(*MMSE_probable_Dementia_thresholder(Y_MMSE, MMSE_threshold_24))\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "# turning everything into numpy arrays because why not\n",
    "\n",
    "df_index = np.asarray(df.index.values)\n",
    "X_id = np.asarray(X_id)\n",
    "X_sex = np.asarray(X_sex)\n",
    "X_sex_binary = np.asarray(X_sex_binary)\n",
    "X_sex_encoded = np.asarray(X_sex_encoded)\n",
    "X_handedness = np.asarray(X_handedness)\n",
    "X_hand_binary = np.asarray(X_hand_binary)\n",
    "X_hand_encoded = np.asarray(X_hand_encoded)\n",
    "X_age = np.asarray(X_age) \n",
    "X_education = np.asarray(X_education) \n",
    "X_SES = np.asarray(X_SES)\n",
    "X_eTIV = np.asarray(X_eTIV)\n",
    "X_nWBV = np.asarray(X_nWBV)\n",
    "X_ASF = np.asarray(X_ASF)\n",
    "axial90loc = np.asarray(axial90loc) \n",
    "axial91loc = np.asarray(axial91loc)\n",
    "axial92loc = np.asarray(axial92loc) \n",
    "axial93loc = np.asarray(axial93loc)\n",
    "axial94loc = np.asarray(axial94loc)\n",
    "axial95loc = np.asarray(axial95loc) \n",
    "axial96loc = np.asarray(axial96loc) \n",
    "axial97loc = np.asarray(axial97loc) \n",
    "axial98loc = np.asarray(axial98loc) \n",
    "axial99loc = np.asarray(axial99loc)\n",
    "axial90_spaghetti = prepPNGimgs(axial90loc)\n",
    "axial91_spaghetti = prepPNGimgs(axial91loc)\n",
    "axial92_spaghetti = prepPNGimgs(axial92loc)\n",
    "axial93_spaghetti = prepPNGimgs(axial93loc)\n",
    "axial94_spaghetti = prepPNGimgs(axial94loc)\n",
    "axial95_spaghetti = prepPNGimgs(axial95loc)\n",
    "axial96_spaghetti = prepPNGimgs(axial96loc)\n",
    "axial97_spaghetti = prepPNGimgs(axial97loc)\n",
    "axial98_spaghetti = prepPNGimgs(axial98loc)\n",
    "axial99_spaghetti = prepPNGimgs(axial99loc)\n",
    "Y_CDR = np.squeeze(np.asarray(Y_CDR))\n",
    "Y_CDR_binary = np.asarray(Y_CDR_binary)\n",
    "Y_CDR_encoded = np.asarray(Y_CDR_encoded)\n",
    "Y_MMSE = np.squeeze(np.asarray(Y_MMSE))\n",
    "Y_MMSE_binary = np.asarray(Y_MMSE_binary)\n",
    "Y_MMSE_encoded = np.asarray(Y_MMSE_encoded)\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "FinalX = np.vstack((X_id, X_sex, X_sex_binary, X_handedness, X_hand_binary, X_age, X_education, X_SES, X_eTIV, X_nWBV, X_ASF))\n",
    "FinalX = FinalX.T\n",
    "\n",
    "FinalY = np.vstack((Y_CDR, Y_CDR_binary, Y_MMSE, Y_MMSE_binary))\n",
    "FinalY = FinalY.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "axial_X_files\n",
    "axial_files0.shape\n",
    "# np.savetxt(\"OASIS_subjs.csv\", axial90loc, delimiter=\",\", fmt='<U61')\n",
    "dt = pd.DataFrame(axial90loc)\n",
    "dt.to_csv(\"OASIS_subjs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "XDATAFRAME = pd.DataFrame((FinalX), index=df_index, columns=['X_id', 'X_sex', 'X_sex_binary', 'X_handedness', 'X_hand_binary', 'X_age', 'X_education', 'X_SES', 'X_eTIV', 'X_nWBV', 'X_ASF'])\n",
    "\n",
    "YDATAFRAME = pd.DataFrame((FinalY), index=df_index, columns=['Y_CDR', 'Y_CDR_binary', 'Y_MMSE', 'Y_MMSE_binary'])\n",
    "\n",
    "classifiers = pd.DataFrame((Y_CDR_encoded), index=df_index, columns=['NC','AD'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "axial90s = pd.concat([pd.DataFrame((axial90_spaghetti), index=df_index), classifiers], axis=1)\n",
    "axial91s = pd.concat([pd.DataFrame((axial91_spaghetti), index=df_index), classifiers], axis=1)\n",
    "axial92s = pd.concat([pd.DataFrame((axial92_spaghetti), index=df_index), classifiers], axis=1)\n",
    "axial93s = pd.concat([pd.DataFrame((axial93_spaghetti), index=df_index), classifiers], axis=1)\n",
    "axial94s = pd.concat([pd.DataFrame((axial94_spaghetti), index=df_index), classifiers], axis=1)\n",
    "axial95s = pd.concat([pd.DataFrame((axial95_spaghetti), index=df_index), classifiers], axis=1)\n",
    "axial96s = pd.concat([pd.DataFrame((axial96_spaghetti), index=df_index), classifiers], axis=1)\n",
    "axial97s = pd.concat([pd.DataFrame((axial97_spaghetti), index=df_index), classifiers], axis=1)\n",
    "axial98s = pd.concat([pd.DataFrame((axial98_spaghetti), index=df_index), classifiers], axis=1)\n",
    "axial99s = pd.concat([pd.DataFrame((axial99_spaghetti), index=df_index), classifiers], axis=1)\n",
    "\n",
    "# axial91s = pd.DataFrame((axial91_spaghetti), index=df_index)\n",
    "# axial92s = pd.DataFrame((axial92_spaghetti), index=df_index)\n",
    "# axial93s = pd.DataFrame((axial93_spaghetti), index=df_index)\n",
    "# axial94s = pd.DataFrame((axial94_spaghetti), index=df_index)\n",
    "# axial95s = pd.DataFrame((axial95_spaghetti), index=df_index)\n",
    "# axial96s = pd.DataFrame((axial96_spaghetti), index=df_index)\n",
    "# axial97s = pd.DataFrame((axial97_spaghetti), index=df_index)\n",
    "# axial98s = pd.DataFrame((axial98_spaghetti), index=df_index)\n",
    "# axial99s = pd.DataFrame((axial99_spaghetti), index=df_index)\n",
    "\n",
    "# divide subject groups into training and test\n",
    "\n",
    "\n",
    "# allSlices = pd.concat([axial90s, axial91s, axial93s, axial94s, axial95s, axial96s, axial97s, axial98s, axial99s],axis=0)\n",
    "\n",
    "# #shuffle rows\n",
    "# allSlicesShuff = allSlices.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "# X_train0 = allSlicesShuff.drop(['NC', 'AD'], axis=1)[0:1000].as_matrix()\n",
    "# y_train0 = allSlicesShuff['AD'][0:1000].as_matrix()\n",
    "\n",
    "# X_test0 = allSlicesShuff.drop(['NC', 'AD'], axis=1)[1000:].as_matrix()\n",
    "# y_test0 = allSlicesShuff['AD'][1000:].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(216, 50178)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "axial90s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# axial90s.shape\n",
    "ind= range(0,216)\n",
    "random.seed(1)\n",
    "train_ind = random.sample(range(0,216),200)\n",
    "\n",
    "axials_train = {}\n",
    "axials_train[1] = axial90s.take(train_ind)\n",
    "axials_train[2] = axial91s.take(train_ind)\n",
    "axials_train[3] = axial92s.take(train_ind)\n",
    "axials_train[4] = axial93s.take(train_ind)\n",
    "axials_train[5] = axial94s.take(train_ind)\n",
    "axials_train[6] = axial95s.take(train_ind)\n",
    "axials_train[7] = axial96s.take(train_ind)\n",
    "axials_train[8] = axial97s.take(train_ind)\n",
    "axials_train[9] = axial98s.take(train_ind)\n",
    "axials_train[10] = axial99s.take(train_ind)\n",
    "\n",
    "\n",
    "test_ind = list(set(ind) - set(train_ind))\n",
    "\n",
    "axials_test={}\n",
    "axials_test[1] = axial90s.take(test_ind)\n",
    "axials_test[2] = axial91s.take(test_ind)\n",
    "axials_test[3] = axial92s.take(test_ind)\n",
    "axials_test[4] = axial93s.take(test_ind)\n",
    "axials_test[5] = axial94s.take(test_ind)\n",
    "axials_test[6] = axial95s.take(test_ind)\n",
    "axials_test[7] = axial96s.take(test_ind)\n",
    "axials_test[8] = axial97s.take(test_ind)\n",
    "axials_test[9] = axial98s.take(test_ind)\n",
    "axials_test[10] = axial99s.take(test_ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = pd.concat(axials_train)\n",
    "testDF = pd.concat(axials_test)\n",
    "\n",
    "X_train0 = trainDF.drop(['NC', 'AD'], axis=1).as_matrix()\n",
    "y_train0 = trainDF['AD'].as_matrix()\n",
    "\n",
    "X_test0 = testDF.drop(['NC', 'AD'], axis=1).as_matrix()\n",
    "y_test0 = testDF['AD'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train0)\n",
    "y_train0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(X_train0[1].reshape(224,224), cmap='gray')\n",
    "def cannyEdge(imgArray):\n",
    "    edgesList = []\n",
    "    for i in range(0, imgArray.shape[0]):\n",
    "        img = imgArray[i].reshape(224,224)\n",
    "\n",
    "        nz = np.nonzero(img)\n",
    "\n",
    "        lower = np.mean(nz)*0.25\n",
    "        upper = np.mean(nz)*0.75\n",
    "\n",
    "        edges= cv2.Canny(img, lower, upper)\n",
    "        flatEdge = edges.flatten()\n",
    "        edgesList.append(flatEdge)\n",
    "\n",
    "    return(np.asarray(edgesList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_edges = cannyEdge(X_train0)\n",
    "X_test_edges = cannyEdge(X_test0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath= '/src/Data/ANIML/'\n",
    "np.save('/src/Data/ANIML/X_train.npy', X_train0)\n",
    "np.save(dataPath+'y_train.npy', y_train0)\n",
    "np.save(dataPath+'X_test.npy', X_test0)\n",
    "np.save(dataPath+'y_test.npy', y_test0)\n",
    "np.save(dataPath+'AX90.npy', axial90s)\n",
    "np.save(dataPath+'AX91.npy', axial91s)\n",
    "np.save(dataPath+'AX92.npy', axial92s)\n",
    "np.save(dataPath+'AX93.npy', axial93s)\n",
    "np.save(dataPath+'AX94.npy', axial94s)\n",
    "np.save(dataPath+'AX95.npy', axial95s)\n",
    "np.save(dataPath+'AX96.npy', axial96s)\n",
    "np.save(dataPath+'AX97.npy', axial97s)\n",
    "np.save(dataPath+'AX98.npy', axial98s)\n",
    "np.save(dataPath+'AX99.npy', axial99s)\n",
    "np.save(dataPath+'X_train_edges.npy', X_train_edges)\n",
    "np.save(dataPath+'X_test_edges.npy', X_test_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 50176)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = X_train0, y_train0, X_test0, y_test0\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNetMnistClassifier():\n",
    "    @staticmethod\n",
    "    def toDigit(hot_encode):\n",
    "        return np.argmax(hot_encode)\n",
    "        \n",
    "    def __init__(self, **kwargs):\n",
    "        if kwargs.get(\"model_path\", None):\n",
    "            from keras.models import load_model\n",
    "            self.model = load_model(kwargs['model_path'])\n",
    "        else:\n",
    "            self.activation_function = kwargs.pop('activation_function', 'relu')\n",
    "            self.batch_size = kwargs.pop('batch_size', 30)\n",
    "            self.epochs = kwargs.pop('epochs', 5)\n",
    "            self.kernal_size = kwargs.pop('kernal_size', (3, 3))\n",
    "            self.loss_function = kwargs.pop('loss_function', 'categorical_crossentropy')\n",
    "            self.optimizer = kwargs.pop('optimizer', 'sgd')\n",
    "            self.pool_size = kwargs.pop('pool_size', (2, 2))\n",
    "            self.model = self._model(X_train, X_train2)\n",
    "            \n",
    "    \n",
    "    def _model(self, X_train, X_train2):\n",
    "        model = Sequential(inputs=[X_train, X_train2])\n",
    "#         Model\n",
    "        model.add(Conv2D(32, kernel_size=self.kernal_size, padding='same',\n",
    "                 activation=self.activation_function,\n",
    "                 input_shape=(224,224,1)))\n",
    "        # One additional convolutional layer (32 channels)\n",
    "        model.add(Conv2D(32, kernel_size=self.kernal_size, padding='same',\n",
    "                 activation=self.activation_function))\n",
    "        model.add(Conv2D(32, kernel_size=self.kernal_size, padding='same',\n",
    "                 activation=self.activation_function))\n",
    "        model.add(MaxPooling2D(pool_size=self.pool_size))\n",
    "        model.add(Conv2D(64, self.kernal_size, padding='same', activation=self.activation_function))\n",
    "        # One additional convolutional layer (64 channels)\n",
    "        model.add(Conv2D(64, self.kernal_size, padding='same', activation=self.activation_function))\n",
    "        model.add(Conv2D(64, self.kernal_size, padding='same', activation=self.activation_function))\n",
    "        model.add(MaxPooling2D(pool_size=self.pool_size))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation=self.activation_function))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(2, activation='softmax'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def predict(self, digits=()):\n",
    "        return self.model.predict(digits)\n",
    "    \n",
    "    def evaluate(self, X_test=None, y_test=None):\n",
    "        X_test = X_test.reshape(X_test.shape[0], 224, 224, 1)\n",
    "        X_test = X_test.astype('float32')\n",
    "        X_test/=255        \n",
    "        number_of_classes = 2\n",
    "        y_test = np_utils.to_categorical(y_test, number_of_classes)\n",
    "        return self.model.evaluate(X_test, y_test)\n",
    "    \n",
    "    def preprocess_and_train(self, X_train=None,X_train2=None, y_train=None, X_test=None, X_test2=None,y_test=None):\n",
    "        self._train(*self._preprocess(X_train, X_train2, y_train, X_test, X_test2,y_test))\n",
    "        \n",
    "    def _preprocess(self, X_train, X_train2, y_train, X_test, X_test2,y_test):\n",
    "        X_train = X_train.reshape(X_train.shape[0], 224, 224, 1)\n",
    "        X_test = X_test.reshape(X_test.shape[0], 224, 224, 1)\n",
    "\n",
    "        X_train = X_train.astype('float32')\n",
    "        X_test = X_test.astype('float32')\n",
    "        \n",
    "        X_train/=255\n",
    "        X_test/=255\n",
    "        \n",
    "        X_train2 = X_train2.reshape(X_train2.shape[0], 224, 224, 1)\n",
    "        X_test2 = X_test2.reshape(X_test2.shape[0], 224, 224, 1)\n",
    "\n",
    "        X_train2 = X_train2.astype('float32')\n",
    "        X_test2 = X_test2.astype('float32')\n",
    "        \n",
    "        X_train2/=255\n",
    "        X_test2/=255\n",
    "        \n",
    "        \n",
    "        number_of_classes = 2\n",
    "        y_train = np_utils.to_categorical(y_train, number_of_classes)\n",
    "        y_test = np_utils.to_categorical(y_test, number_of_classes)\n",
    "        \n",
    "        return X_train, X_train2, y_train, X_test, X_test2, y_test\n",
    "    \n",
    "    def _train(self, X_train, X_train2, y_train, X_test, X_test2, y_test):\n",
    "        self.model.compile(loss=self.loss_function,\n",
    "              optimizer=self.optimizer,\n",
    "              metrics=['accuracy'])\n",
    "        \n",
    "        fit_output = self.model.fit([X_train, X_train2],\n",
    "                        y_train,\n",
    "                        batch_size=self.batch_size,\n",
    "                        epochs=self.epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=([X_test, X_test2], y_test))\n",
    "        self._history = fit_output.history\n",
    "        \n",
    "        import time\n",
    "        timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        self.model.save(\"oasis_test_{0}.h5\".format(timestr)) \n",
    "    \n",
    "    @property\n",
    "    def history(self):\n",
    "        return self._history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-f1bf3645b740>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLeNetMnistClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train_edges\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdadelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_and_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train_edges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_test0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_test_edges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_test0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-291485dc8b7e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sgd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pool_size'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train2' is not defined"
     ]
    }
   ],
   "source": [
    "classifier = LeNetMnistClassifier(X_train=X_train0, X_train2=X_train_edges,epochs=30, optimizer=keras.optimizers.Adadelta())\n",
    "classifier.preprocess_and_train(X_train=X_train0, X_train2=X_train_edges, y_train=y_train0, X_test=X_test0, X_test2=X_test_edges, y_test=y_test0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 73s 37ms/step - loss: 0.6785 - acc: 0.6035 - val_loss: 0.6717 - val_acc: 0.6250\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 19s 9ms/step - loss: 0.6708 - acc: 0.6150 - val_loss: 0.6624 - val_acc: 0.6250\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 19s 9ms/step - loss: 0.6678 - acc: 0.6150 - val_loss: 0.6613 - val_acc: 0.6250\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 19s 9ms/step - loss: 0.6680 - acc: 0.6150 - val_loss: 0.6614 - val_acc: 0.6250\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 19s 9ms/step - loss: 0.6676 - acc: 0.6150 - val_loss: 0.6618 - val_acc: 0.6250\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 19s 9ms/step - loss: 0.6672 - acc: 0.6150 - val_loss: 0.6596 - val_acc: 0.6250\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 19s 9ms/step - loss: 0.6691 - acc: 0.6155 - val_loss: 0.6612 - val_acc: 0.6250\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 19s 9ms/step - loss: 0.6668 - acc: 0.6170 - val_loss: 0.6624 - val_acc: 0.6250\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 0.6672 - acc: 0.6150 - val_loss: 0.6616 - val_acc: 0.6250\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 0.6672 - acc: 0.6150 - val_loss: 0.6619 - val_acc: 0.6250\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 0.6673 - acc: 0.6150 - val_loss: 0.6616 - val_acc: 0.6250\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 18s 9ms/step - loss: 0.6670 - acc: 0.6150 - val_loss: 0.6631 - val_acc: 0.6250\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 19s 9ms/step - loss: 0.6667 - acc: 0.6150 - val_loss: 0.6619 - val_acc: 0.6250\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 19s 9ms/step - loss: 0.6638 - acc: 0.6200 - val_loss: 0.6608 - val_acc: 0.6250\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 19s 9ms/step - loss: 0.6401 - acc: 0.6330 - val_loss: 0.6027 - val_acc: 0.6688\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 19s 9ms/step - loss: 0.6044 - acc: 0.6825 - val_loss: 0.6602 - val_acc: 0.6250\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 19s 10ms/step - loss: 0.5405 - acc: 0.7290 - val_loss: 0.6654 - val_acc: 0.5813\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 19s 9ms/step - loss: 0.4299 - acc: 0.8050 - val_loss: 0.9378 - val_acc: 0.5438\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 19s 10ms/step - loss: 0.2720 - acc: 0.8795 - val_loss: 1.5046 - val_acc: 0.6250\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 19s 9ms/step - loss: 0.1085 - acc: 0.9570 - val_loss: 1.7478 - val_acc: 0.6125\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 19s 9ms/step - loss: 0.0603 - acc: 0.9780 - val_loss: 1.5183 - val_acc: 0.5563\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 19s 9ms/step - loss: 0.0160 - acc: 0.9965 - val_loss: 2.6372 - val_acc: 0.5813\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 19s 9ms/step - loss: 0.0146 - acc: 0.9945 - val_loss: 3.1161 - val_acc: 0.6000\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 19s 9ms/step - loss: 0.0048 - acc: 0.9980 - val_loss: 3.2225 - val_acc: 0.6125\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 19s 9ms/step - loss: 0.0120 - acc: 0.9945 - val_loss: 3.1327 - val_acc: 0.5500\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 19s 9ms/step - loss: 0.0072 - acc: 0.9985 - val_loss: 3.1156 - val_acc: 0.5813\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 19s 9ms/step - loss: 0.0045 - acc: 0.9980 - val_loss: 3.5216 - val_acc: 0.5625\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 19s 10ms/step - loss: 0.0041 - acc: 0.9985 - val_loss: 3.7862 - val_acc: 0.6063\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 19s 9ms/step - loss: 5.7771e-04 - acc: 1.0000 - val_loss: 3.9434 - val_acc: 0.5250\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 19s 9ms/step - loss: 0.0205 - acc: 0.9970 - val_loss: 3.1080 - val_acc: 0.6000\n"
     ]
    }
   ],
   "source": [
    "classifier = LeNetMnistClassifier(epochs=30, optimizer=keras.optimizers.Adadelta())\n",
    "classifier.preprocess_and_train(X_train=X_train0, y_train=y_train0, X_test=X_test0, y_test=y_test0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 13154147634325830913\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 104136704\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 18100578659675994095\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0\"\n",
      "]\n",
      "1.2.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare data set\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
