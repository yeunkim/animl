{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://pypi.python.org/packages/7b/98/7445165b69d4e95403372c07845ad1756af509ac85fc33c5b88e1c3f90c9/xlrd-0.9.0.tar.gz#md5=61102459833cc31d6b05404325fa45a8\n",
      "  Looking up \"https://pypi.python.org/packages/7b/98/7445165b69d4e95403372c07845ad1756af509ac85fc33c5b88e1c3f90c9/xlrd-0.9.0.tar.gz\" in the cache\n",
      "  No cache entry available\n",
      "  Starting new HTTPS connection (1): pypi.python.org\n",
      "  \"GET /packages/7b/98/7445165b69d4e95403372c07845ad1756af509ac85fc33c5b88e1c3f90c9/xlrd-0.9.0.tar.gz HTTP/1.1\" 200 134993\n",
      "  Downloading xlrd-0.9.0.tar.gz (134kB)\n",
      "\u001b[?25l  Downloading from URL https://pypi.python.org/packages/7b/98/7445165b69d4e95403372c07845ad1756af509ac85fc33c5b88e1c3f90c9/xlrd-0.9.0.tar.gz#md5=61102459833cc31d6b05404325fa45a8\n",
      "\u001b[K    98% |############################### | 133kB 13.2MB/s eta 0:00:01  Updating cache with response from \"https://pypi.python.org/packages/7b/98/7445165b69d4e95403372c07845ad1756af509ac85fc33c5b88e1c3f90c9/xlrd-0.9.0.tar.gz\"\n",
      "  Caching due to etag\n",
      "\u001b[K    100% |################################| 143kB 2.6MB/s \n",
      "\u001b[?25h  Running setup.py (path:/tmp/pip-ucxguz41-build/setup.py) egg_info for package from https://pypi.python.org/packages/7b/98/7445165b69d4e95403372c07845ad1756af509ac85fc33c5b88e1c3f90c9/xlrd-0.9.0.tar.gz#md5=61102459833cc31d6b05404325fa45a8\n",
      "    Running command python setup.py egg_info\n",
      "    running egg_info\n",
      "    creating pip-egg-info/xlrd.egg-info\n",
      "    writing pip-egg-info/xlrd.egg-info/PKG-INFO\n",
      "    writing top-level names to pip-egg-info/xlrd.egg-info/top_level.txt\n",
      "    writing dependency_links to pip-egg-info/xlrd.egg-info/dependency_links.txt\n",
      "    writing manifest file 'pip-egg-info/xlrd.egg-info/SOURCES.txt'\n",
      "    reading manifest file 'pip-egg-info/xlrd.egg-info/SOURCES.txt'\n",
      "    writing manifest file 'pip-egg-info/xlrd.egg-info/SOURCES.txt'\n",
      "  Source in /tmp/pip-ucxguz41-build has version 0.9.0, which satisfies requirement xlrd==0.9.0 from https://pypi.python.org/packages/7b/98/7445165b69d4e95403372c07845ad1756af509ac85fc33c5b88e1c3f90c9/xlrd-0.9.0.tar.gz#md5=61102459833cc31d6b05404325fa45a8\n",
      "Building wheels for collected packages: xlrd\n",
      "  Running setup.py bdist_wheel for xlrd ... \u001b[?25l  Destination directory: /tmp/tmpjst7qebbpip-wheel-\n",
      "  Running command /root/miniconda3/envs/carnd-term1/bin/python -u -c \"import setuptools, tokenize;__file__='/tmp/pip-ucxguz41-build/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d /tmp/tmpjst7qebbpip-wheel- --python-tag cp35\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib\n",
      "  creating build/lib/xlrd\n",
      "  copying xlrd/info.py -> build/lib/xlrd\n",
      "  copying xlrd/timemachine.py -> build/lib/xlrd\n",
      "  copying xlrd/formatting.py -> build/lib/xlrd\n",
      "  copying xlrd/sheet.py -> build/lib/xlrd\n",
      "  copying xlrd/xldate.py -> build/lib/xlrd\n",
      "  copying xlrd/__init__.py -> build/lib/xlrd\n",
      "  copying xlrd/biffh.py -> build/lib/xlrd\n",
      "  copying xlrd/formula.py -> build/lib/xlrd\n",
      "  copying xlrd/xlsx.py -> build/lib/xlrd\n",
      "  copying xlrd/compdoc.py -> build/lib/xlrd\n",
      "  copying xlrd/book.py -> build/lib/xlrd\n",
      "  copying xlrd/licences.py -> build/lib/xlrd\n",
      "  creating build/lib/xlrd/doc\n",
      "  copying xlrd/doc/xlrd.html -> build/lib/xlrd/doc\n",
      "  copying xlrd/doc/compdoc.html -> build/lib/xlrd/doc\n",
      "  creating build/lib/xlrd/examples\n",
      "  copying xlrd/examples/namesdemo.xls -> build/lib/xlrd/examples\n",
      "  copying xlrd/examples/xlrdnameAPIdemo.py -> build/lib/xlrd/examples\n",
      "  running build_scripts\n",
      "  creating build/scripts-3.5\n",
      "  copying and adjusting scripts/runxlrd.py -> build/scripts-3.5\n",
      "  changing mode of build/scripts-3.5/runxlrd.py from 644 to 755\n",
      "  installing to build/bdist.linux-x86_64/wheel\n",
      "  running install\n",
      "  running install_lib\n",
      "  creating build/bdist.linux-x86_64\n",
      "  creating build/bdist.linux-x86_64/wheel\n",
      "  creating build/bdist.linux-x86_64/wheel/xlrd\n",
      "  copying build/lib/xlrd/info.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  creating build/bdist.linux-x86_64/wheel/xlrd/doc\n",
      "  copying build/lib/xlrd/doc/xlrd.html -> build/bdist.linux-x86_64/wheel/xlrd/doc\n",
      "  copying build/lib/xlrd/doc/compdoc.html -> build/bdist.linux-x86_64/wheel/xlrd/doc\n",
      "  copying build/lib/xlrd/timemachine.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  copying build/lib/xlrd/formatting.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  copying build/lib/xlrd/sheet.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  copying build/lib/xlrd/xldate.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  copying build/lib/xlrd/__init__.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  copying build/lib/xlrd/biffh.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  copying build/lib/xlrd/formula.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  copying build/lib/xlrd/xlsx.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  copying build/lib/xlrd/compdoc.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  creating build/bdist.linux-x86_64/wheel/xlrd/examples\n",
      "  copying build/lib/xlrd/examples/namesdemo.xls -> build/bdist.linux-x86_64/wheel/xlrd/examples\n",
      "  copying build/lib/xlrd/examples/xlrdnameAPIdemo.py -> build/bdist.linux-x86_64/wheel/xlrd/examples\n",
      "  copying build/lib/xlrd/book.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  copying build/lib/xlrd/licences.py -> build/bdist.linux-x86_64/wheel/xlrd\n",
      "  running install_egg_info\n",
      "  running egg_info\n",
      "  creating xlrd.egg-info\n",
      "  writing dependency_links to xlrd.egg-info/dependency_links.txt\n",
      "  writing top-level names to xlrd.egg-info/top_level.txt\n",
      "  writing xlrd.egg-info/PKG-INFO\n",
      "  writing manifest file 'xlrd.egg-info/SOURCES.txt'\n",
      "  reading manifest file 'xlrd.egg-info/SOURCES.txt'\n",
      "  writing manifest file 'xlrd.egg-info/SOURCES.txt'\n",
      "  Copying xlrd.egg-info to build/bdist.linux-x86_64/wheel/xlrd-0.9.0-py3.5.egg-info\n",
      "  running install_scripts\n",
      "  creating build/bdist.linux-x86_64/wheel/xlrd-0.9.0.data\n",
      "  creating build/bdist.linux-x86_64/wheel/xlrd-0.9.0.data/scripts\n",
      "  copying build/scripts-3.5/runxlrd.py -> build/bdist.linux-x86_64/wheel/xlrd-0.9.0.data/scripts\n",
      "  changing mode of build/bdist.linux-x86_64/wheel/xlrd-0.9.0.data/scripts/runxlrd.py to 755\n",
      "  creating build/bdist.linux-x86_64/wheel/xlrd-0.9.0.dist-info/WHEEL\n",
      "  creating '/tmp/tmpjst7qebbpip-wheel-/xlrd-0.9.0-cp35-none-any.whl' and adding '.' to it\n",
      "  adding 'xlrd/__init__.py'\n",
      "  adding 'xlrd/biffh.py'\n",
      "  adding 'xlrd/book.py'\n",
      "  adding 'xlrd/compdoc.py'\n",
      "  adding 'xlrd/formatting.py'\n",
      "  adding 'xlrd/formula.py'\n",
      "  adding 'xlrd/info.py'\n",
      "  adding 'xlrd/licences.py'\n",
      "  adding 'xlrd/sheet.py'\n",
      "  adding 'xlrd/timemachine.py'\n",
      "  adding 'xlrd/xldate.py'\n",
      "  adding 'xlrd/xlsx.py'\n",
      "  adding 'xlrd/doc/compdoc.html'\n",
      "  adding 'xlrd/doc/xlrd.html'\n",
      "  adding 'xlrd/examples/namesdemo.xls'\n",
      "  adding 'xlrd/examples/xlrdnameAPIdemo.py'\n",
      "  adding 'xlrd-0.9.0.data/scripts/runxlrd.py'\n",
      "  adding 'xlrd-0.9.0.dist-info/DESCRIPTION.rst'\n",
      "  adding 'xlrd-0.9.0.dist-info/metadata.json'\n",
      "  adding 'xlrd-0.9.0.dist-info/top_level.txt'\n",
      "  adding 'xlrd-0.9.0.dist-info/WHEEL'\n",
      "  adding 'xlrd-0.9.0.dist-info/METADATA'\n",
      "  adding 'xlrd-0.9.0.dist-info/RECORD'\n",
      "done\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/f4/0c/09/39986983b4a793686e129a320cc5c39cf14f6ce6067f961a8d\n",
      "  Removing source in /tmp/pip-ucxguz41-build\n",
      "Successfully built xlrd\n",
      "Installing collected packages: xlrd\n",
      "\n",
      "Successfully installed xlrd-0.9.0\n",
      "Cleaning up...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!pip install -Iv https://pypi.python.org/packages/7b/98/7445165b69d4e95403372c07845ad1756af509ac85fc33c5b88e1c3f90c9/xlrd-0.9.0.tar.gz#md5=61102459833cc31d6b05404325fa45a8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/pandas/util/_decorators.py:118: FutureWarning: The `sheetname` keyword is deprecated, use `sheet_name` instead\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "import pandas as pd\n",
    "import os \n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "demographics = pd.read_excel('/src/Data/ANIML/oasis_cross-sectional.xls', sheetname=3) # load data\n",
    "df = demographics.dropna(how='any') # remove NaN values\n",
    "df_columns = list(demographics.columns)\n",
    "X_columns = np.delete(df_columns, [6,7], None) # X matrix won't have MMSE or CDR scores\n",
    "Xdf = df.reindex(columns=X_columns)\n",
    "X = Xdf.values # creating X\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "# Define function to get list of pngs based on slice number\n",
    "pngs_path='/src/Data/ANIML/OASIS_pngs'\n",
    "def getaxialPNG(path):\n",
    "    l = []\n",
    "    axialslice90_files = []\n",
    "    axialslice91_files = []\n",
    "    axialslice92_files = []\n",
    "    axialslice93_files = []\n",
    "    axialslice94_files = []\n",
    "    axialslice95_files = []\n",
    "    axialslice96_files = []\n",
    "    axialslice97_files = []\n",
    "    axialslice98_files = []\n",
    "    axialslice99_files = []\n",
    "    \n",
    "    for root, directories, filenames in os.walk(path):\n",
    "        \n",
    "        for filename in filenames:\n",
    "            if \".90.\" in filename: \n",
    "                axialslice90_files.append(os.path.join(root, filename))\n",
    "            if \".91.\" in filename: \n",
    "                axialslice91_files.append(os.path.join(root, filename))\n",
    "            if \".92.\" in filename: \n",
    "                axialslice92_files.append(os.path.join(root, filename))\n",
    "            if \".93.\" in filename: \n",
    "                axialslice93_files.append(os.path.join(root, filename))\n",
    "            if \".94.\" in filename: \n",
    "                axialslice94_files.append(os.path.join(root, filename))\n",
    "            if \".95.\" in filename: \n",
    "                axialslice95_files.append(os.path.join(root, filename))\n",
    "            if \".96.\" in filename: \n",
    "                axialslice96_files.append(os.path.join(root, filename))\n",
    "            if \".97.\" in filename: \n",
    "                axialslice97_files.append(os.path.join(root, filename))\n",
    "            if \".98.\" in filename: \n",
    "                axialslice98_files.append(os.path.join(root, filename))\n",
    "            if \".99.\" in filename: \n",
    "                axialslice99_files.append(os.path.join(root, filename))\n",
    "\n",
    "    l = list(zip(axialslice90_files, axialslice91_files, axialslice92_files, axialslice93_files, axialslice94_files, axialslice95_files, axialslice96_files, axialslice97_files, axialslice98_files, axialslice99_files))\n",
    "\n",
    "    return ((np.asarray(l)))\n",
    "\n",
    "axial_files0 = getaxialPNG(pngs_path)\n",
    "axial_X_files = np.take(axial_files0, indices=df.index.values, axis=0) # keeps the images with the same index as X matrix\n",
    "axial90loc, axial91loc, axial92loc, axial93loc, axial94loc, axial95loc, axial96loc, axial97loc, axial98loc, axial99loc = zip(*axial_X_files)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "X_id, X_sex, X_handedness, X_age, X_education, X_SES, X_eTIV, X_nWBV, X_ASF = zip(*X) # unzips big X matrix\n",
    "\n",
    "def sex_translator(X_sex):\n",
    "    X_sex_binary = []\n",
    "    X_sex_encoded = []\n",
    "    for x in X_sex:\n",
    "        if x == 'M':\n",
    "            X_sex_binary.append(1)\n",
    "            X_sex_encoded.append([0,1])\n",
    "        else:\n",
    "            X_sex_binary.append(-1)\n",
    "            X_sex_encoded.append([1,0])\n",
    "    \n",
    "    return(zip(X_sex_binary, X_sex_encoded)) # gives us binary and one-hot encoded for sex\n",
    "           \n",
    "def hand_translator(X_handedness):\n",
    "    X_hand_binary = []\n",
    "    X_hand_encoded = []\n",
    "    for x in X_handedness:\n",
    "        if x == 'R':\n",
    "            X_hand_binary.append(1)\n",
    "            X_hand_encoded.append([0,1])\n",
    "        else:\n",
    "            X_hand_binary.append(-1)\n",
    "            X_hand_encoded.append([1,0])\n",
    "    \n",
    "    return(zip(X_hand_binary, X_hand_encoded)) # same as above but for handedness\n",
    "\n",
    "# turns out all the patients are right-handed\n",
    "# maybe we should all just be left-handed so we don't suffer from Alzheimer's\n",
    "# /frequentist sarcasm\n",
    "\n",
    "X_sex_binary, X_sex_encoded = zip(*sex_translator(X_sex)) # unzipping to get our function outputs\n",
    "X_hand_binary, X_hand_encoded = zip(*hand_translator(X_handedness))\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "def prepPNGimgs(array_of_image_paths):\n",
    "    l = []\n",
    "    for img_file in array_of_image_paths: # for each file in the list of images...\n",
    "        img = cv2.imread(\"{}\".format(img_file)) # read the image...\n",
    "        img = cv2.resize(np.array(img), (224,224)) # resize it to 224 by 224 QUICK WHAT'S 224 SQUARED??\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # makes it grayscale\n",
    "        flatten = gray.flatten() # spaghettifies it into 1 by 50176 (which is 224 squared)\n",
    "        l.append(flatten)\n",
    "    \n",
    "    return(np.asarray(l))\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "Y_CDR_columns = [column_name for column_name in df_columns if column_name == 'CDR']\n",
    "Y_CDR_df = df.reindex(columns=Y_CDR_columns)\n",
    "Y_CDR = Y_CDR_df.values\n",
    "\n",
    "Y_MMSE_columns = [column_name for column_name in df_columns if column_name == 'MMSE']\n",
    "Y_MMSE_df = df.reindex(columns=Y_MMSE_columns)\n",
    "Y_MMSE = Y_MMSE_df.values\n",
    "\n",
    "CDR_threshold_0 = 0 # threshold values by CDR scale\n",
    "CDR_threshold_0point5 = 0.5\n",
    "CDR_threshold_1 = 1\n",
    "\n",
    "MMSE_threshold_24 = 24 # threshold values by MMSE scale\n",
    "MMSE_threshold_18 = 18\n",
    "\n",
    "def CDR_probable_AD_thresholder(Y_CDR, threshold_value):\n",
    "    Y_CDR_binary = []\n",
    "    Y_CDR_encoded = []\n",
    "    for y in Y_CDR:\n",
    "        if y > threshold_value:\n",
    "            Y_CDR_binary.append(1)\n",
    "            Y_CDR_encoded.append([0,1])\n",
    "        else:\n",
    "            Y_CDR_binary.append(-1)\n",
    "            Y_CDR_encoded.append([1,0])\n",
    "\n",
    "    return((zip(Y_CDR_binary, Y_CDR_encoded)))\n",
    "\n",
    "def MMSE_probable_Dementia_thresholder(Y_MMSE, threshold_value):\n",
    "    Y_MMSE_binary = []\n",
    "    Y_MMSE_encoded = []\n",
    "    for y in Y_MMSE:\n",
    "        if y < threshold_value:\n",
    "            Y_MMSE_binary.append(1)\n",
    "            Y_MMSE_encoded.append([0,1])\n",
    "        else:\n",
    "            Y_MMSE_binary.append(-1)\n",
    "            Y_MMSE_encoded.append([1,0])\n",
    "        \n",
    "    return(zip(Y_MMSE_binary, Y_MMSE_encoded))\n",
    "\n",
    "Y_CDR_binary, Y_CDR_encoded = zip(*CDR_probable_AD_thresholder(Y_CDR, CDR_threshold_0))\n",
    "Y_MMSE_binary, Y_MMSE_encoded = zip(*MMSE_probable_Dementia_thresholder(Y_MMSE, MMSE_threshold_24))\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "# turning everything into numpy arrays because why not\n",
    "\n",
    "df_index = np.asarray(df.index.values)\n",
    "X_id = np.asarray(X_id)\n",
    "X_sex = np.asarray(X_sex)\n",
    "X_sex_binary = np.asarray(X_sex_binary)\n",
    "X_sex_encoded = np.asarray(X_sex_encoded)\n",
    "X_handedness = np.asarray(X_handedness)\n",
    "X_hand_binary = np.asarray(X_hand_binary)\n",
    "X_hand_encoded = np.asarray(X_hand_encoded)\n",
    "X_age = np.asarray(X_age) \n",
    "X_education = np.asarray(X_education) \n",
    "X_SES = np.asarray(X_SES)\n",
    "X_eTIV = np.asarray(X_eTIV)\n",
    "X_nWBV = np.asarray(X_nWBV)\n",
    "X_ASF = np.asarray(X_ASF)\n",
    "axial90loc = np.asarray(axial90loc) \n",
    "axial91loc = np.asarray(axial91loc)\n",
    "axial92loc = np.asarray(axial92loc) \n",
    "axial93loc = np.asarray(axial93loc)\n",
    "axial94loc = np.asarray(axial94loc)\n",
    "axial95loc = np.asarray(axial95loc) \n",
    "axial96loc = np.asarray(axial96loc) \n",
    "axial97loc = np.asarray(axial97loc) \n",
    "axial98loc = np.asarray(axial98loc) \n",
    "axial99loc = np.asarray(axial99loc)\n",
    "axial90_spaghetti = prepPNGimgs(axial90loc)\n",
    "axial91_spaghetti = prepPNGimgs(axial91loc)\n",
    "axial92_spaghetti = prepPNGimgs(axial92loc)\n",
    "axial93_spaghetti = prepPNGimgs(axial93loc)\n",
    "axial94_spaghetti = prepPNGimgs(axial94loc)\n",
    "axial95_spaghetti = prepPNGimgs(axial95loc)\n",
    "axial96_spaghetti = prepPNGimgs(axial96loc)\n",
    "axial97_spaghetti = prepPNGimgs(axial97loc)\n",
    "axial98_spaghetti = prepPNGimgs(axial98loc)\n",
    "axial99_spaghetti = prepPNGimgs(axial99loc)\n",
    "Y_CDR = np.squeeze(np.asarray(Y_CDR))\n",
    "Y_CDR_binary = np.asarray(Y_CDR_binary)\n",
    "Y_CDR_encoded = np.asarray(Y_CDR_encoded)\n",
    "Y_MMSE = np.squeeze(np.asarray(Y_MMSE))\n",
    "Y_MMSE_binary = np.asarray(Y_MMSE_binary)\n",
    "Y_MMSE_encoded = np.asarray(Y_MMSE_encoded)\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "FinalX = np.vstack((X_id, X_sex, X_sex_binary, X_handedness, X_hand_binary, X_age, X_education, X_SES, X_eTIV, X_nWBV, X_ASF))\n",
    "FinalX = FinalX.T\n",
    "\n",
    "FinalY = np.vstack((Y_CDR, Y_CDR_binary, Y_MMSE, Y_MMSE_binary))\n",
    "FinalY = FinalY.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "XDATAFRAME = pd.DataFrame((FinalX), index=df_index, columns=['X_id', 'X_sex', 'X_sex_binary', 'X_handedness', 'X_hand_binary', 'X_age', 'X_education', 'X_SES', 'X_eTIV', 'X_nWBV', 'X_ASF'])\n",
    "\n",
    "YDATAFRAME = pd.DataFrame((FinalY), index=df_index, columns=['Y_CDR', 'Y_CDR_binary', 'Y_MMSE', 'Y_MMSE_binary'])\n",
    "\n",
    "classifiers = pd.DataFrame((Y_CDR_encoded), index=df_index, columns=['NC','AD'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "axial90s = pd.concat([pd.DataFrame((axial90_spaghetti), index=df_index), classifiers], axis=1)\n",
    "axial91s = pd.concat([pd.DataFrame((axial91_spaghetti), index=df_index), classifiers], axis=1)\n",
    "axial92s = pd.concat([pd.DataFrame((axial92_spaghetti), index=df_index), classifiers], axis=1)\n",
    "axial93s = pd.concat([pd.DataFrame((axial93_spaghetti), index=df_index), classifiers], axis=1)\n",
    "axial94s = pd.concat([pd.DataFrame((axial94_spaghetti), index=df_index), classifiers], axis=1)\n",
    "axial95s = pd.concat([pd.DataFrame((axial95_spaghetti), index=df_index), classifiers], axis=1)\n",
    "axial96s = pd.concat([pd.DataFrame((axial96_spaghetti), index=df_index), classifiers], axis=1)\n",
    "axial97s = pd.concat([pd.DataFrame((axial97_spaghetti), index=df_index), classifiers], axis=1)\n",
    "axial98s = pd.concat([pd.DataFrame((axial98_spaghetti), index=df_index), classifiers], axis=1)\n",
    "axial99s = pd.concat([pd.DataFrame((axial99_spaghetti), index=df_index), classifiers], axis=1)\n",
    "\n",
    "# axial91s = pd.DataFrame((axial91_spaghetti), index=df_index)\n",
    "# axial92s = pd.DataFrame((axial92_spaghetti), index=df_index)\n",
    "# axial93s = pd.DataFrame((axial93_spaghetti), index=df_index)\n",
    "# axial94s = pd.DataFrame((axial94_spaghetti), index=df_index)\n",
    "# axial95s = pd.DataFrame((axial95_spaghetti), index=df_index)\n",
    "# axial96s = pd.DataFrame((axial96_spaghetti), index=df_index)\n",
    "# axial97s = pd.DataFrame((axial97_spaghetti), index=df_index)\n",
    "# axial98s = pd.DataFrame((axial98_spaghetti), index=df_index)\n",
    "# axial99s = pd.DataFrame((axial99_spaghetti), index=df_index)\n",
    "\n",
    "# divide subject groups into training and test\n",
    "\n",
    "\n",
    "# allSlices = pd.concat([axial90s, axial91s, axial93s, axial94s, axial95s, axial96s, axial97s, axial98s, axial99s],axis=0)\n",
    "\n",
    "# #shuffle rows\n",
    "# allSlicesShuff = allSlices.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "# X_train0 = allSlicesShuff.drop(['NC', 'AD'], axis=1)[0:1000].as_matrix()\n",
    "# y_train0 = allSlicesShuff['AD'][0:1000].as_matrix()\n",
    "\n",
    "# X_test0 = allSlicesShuff.drop(['NC', 'AD'], axis=1)[1000:].as_matrix()\n",
    "# y_test0 = allSlicesShuff['AD'][1000:].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# axial90s.shape\n",
    "ind= range(0,216)\n",
    "random.seed(1)\n",
    "train_ind = random.sample(range(0,216),200)\n",
    "\n",
    "axials_train = {}\n",
    "axials_train[1] = axial90s.take(train_ind)\n",
    "axials_train[2] = axial91s.take(train_ind)\n",
    "axials_train[3] = axial92s.take(train_ind)\n",
    "axials_train[4] = axial93s.take(train_ind)\n",
    "axials_train[5] = axial94s.take(train_ind)\n",
    "axials_train[6] = axial95s.take(train_ind)\n",
    "axials_train[7] = axial96s.take(train_ind)\n",
    "axials_train[8] = axial97s.take(train_ind)\n",
    "axials_train[9] = axial98s.take(train_ind)\n",
    "axials_train[10] = axial99s.take(train_ind)\n",
    "\n",
    "\n",
    "test_ind = list(set(ind) - set(train_ind))\n",
    "\n",
    "axials_test={}\n",
    "axials_test[1] = axial90s.take(test_ind)\n",
    "axials_test[2] = axial91s.take(test_ind)\n",
    "axials_test[3] = axial92s.take(test_ind)\n",
    "axials_test[4] = axial93s.take(test_ind)\n",
    "axials_test[5] = axial94s.take(test_ind)\n",
    "axials_test[6] = axial95s.take(test_ind)\n",
    "axials_test[7] = axial96s.take(test_ind)\n",
    "axials_test[8] = axial97s.take(test_ind)\n",
    "axials_test[9] = axial98s.take(test_ind)\n",
    "axials_test[10] = axial99s.take(test_ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = pd.concat(axials_train)\n",
    "testDF = pd.concat(axials_test)\n",
    "\n",
    "X_train0 = trainDF.drop(['NC', 'AD'], axis=1).as_matrix()\n",
    "y_train0 = trainDF['AD'].as_matrix()\n",
    "\n",
    "X_test0 = testDF.drop(['NC', 'AD'], axis=1).as_matrix()\n",
    "y_test0 = testDF['AD'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 50176)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = X_train0, y_train0, X_test0, y_test0\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNetMnistClassifier():\n",
    "    @staticmethod\n",
    "    def toDigit(hot_encode):\n",
    "        return np.argmax(hot_encode)\n",
    "        \n",
    "    def __init__(self, **kwargs):\n",
    "        if kwargs.get(\"model_path\", None):\n",
    "            from keras.models import load_model\n",
    "            self.model = load_model(kwargs['model_path'])\n",
    "        else:\n",
    "            self.activation_function = kwargs.pop('activation_function', 'relu')\n",
    "            self.batch_size = kwargs.pop('batch_size', 30)\n",
    "            self.epochs = kwargs.pop('epochs', 5)\n",
    "            self.kernal_size = kwargs.pop('kernal_size', (3, 3))\n",
    "            self.loss_function = kwargs.pop('loss_function', 'categorical_crossentropy')\n",
    "            self.optimizer = kwargs.pop('optimizer', 'sgd')\n",
    "            self.pool_size = kwargs.pop('pool_size', (2, 2))\n",
    "            self.model = self._model()\n",
    "            \n",
    "    \n",
    "    def _model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, self.kernal_size[0],self.kernal_size[1] , border_mode='same',\n",
    "                 activation=self.activation_function,\n",
    "                 input_shape=(224,224,1)))\n",
    "        # One additional convolutional layer (32 channels)\n",
    "        model.add(Conv2D(32, self.kernal_size[0],self.kernal_size[1], border_mode='same',\n",
    "                 activation=self.activation_function))\n",
    "        model.add(Conv2D(32, self.kernal_size[0],self.kernal_size[1], border_mode='same',\n",
    "                 activation=self.activation_function))\n",
    "        model.add(MaxPooling2D(pool_size=self.pool_size))\n",
    "        model.add(Conv2D(64, self.kernal_size[0],self.kernal_size[1], border_mode='same', activation=self.activation_function))\n",
    "        # One additional convolutional layer (64 channels)\n",
    "        model.add(Conv2D(64, self.kernal_size[0],self.kernal_size[1], border_mode='same', activation=self.activation_function))\n",
    "        model.add(Conv2D(64, self.kernal_size[0],self.kernal_size[1], border_mode='same', activation=self.activation_function))\n",
    "        model.add(MaxPooling2D(pool_size=self.pool_size))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(256, activation=self.activation_function))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(2, activation='softmax'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def predict(self, digits=()):\n",
    "        return self.model.predict(digits)\n",
    "    \n",
    "    def evaluate(self, X_test=None, y_test=None):\n",
    "        X_test = X_test.reshape(X_test.shape[0], 224, 224, 1)\n",
    "        X_test = X_test.astype('float32')\n",
    "        X_test/=255        \n",
    "        number_of_classes = 2\n",
    "        y_test = np_utils.to_categorical(y_test, number_of_classes)\n",
    "        return self.model.evaluate(X_test, y_test)\n",
    "    \n",
    "    def preprocess_and_train(self, X_train=None, y_train=None, X_test=None, y_test=None):\n",
    "        self._train(*self._preprocess(X_train, y_train, X_test, y_test))\n",
    "        \n",
    "    def _preprocess(self, X_train, y_train, X_test, y_test):\n",
    "        X_train = X_train.reshape(X_train.shape[0], 224, 224, 1)\n",
    "        X_test = X_test.reshape(X_test.shape[0], 224, 224, 1)\n",
    "\n",
    "        X_train = X_train.astype('float32')\n",
    "        X_test = X_test.astype('float32')\n",
    "        \n",
    "        X_train/=255\n",
    "        X_test/=255\n",
    "        \n",
    "        number_of_classes = 2\n",
    "        y_train = np_utils.to_categorical(y_train, number_of_classes)\n",
    "        y_test = np_utils.to_categorical(y_test, number_of_classes)\n",
    "        \n",
    "        return X_train, y_train, X_test, y_test\n",
    "    \n",
    "    def _train(self, X_train, y_train, X_test, y_test):\n",
    "        self.model.compile(loss=self.loss_function,\n",
    "              optimizer=self.optimizer,\n",
    "              metrics=['accuracy'])\n",
    "        \n",
    "        fit_output = self.model.fit(X_train,\n",
    "                        y_train,\n",
    "                        batch_size=self.batch_size,\n",
    "                        nb_epoch=self.epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(X_test, y_test))\n",
    "        self._history = fit_output.history\n",
    "        \n",
    "        import time\n",
    "        timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        self.model.save(\"oasis_test_{0}.h5\".format(timestr)) \n",
    "    \n",
    "    @property\n",
    "    def history(self):\n",
    "        return self._history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2000 samples, validate on 160 samples\n",
      "Epoch 1/30\n",
      "2000/2000 [==============================] - 16s - loss: 0.6794 - acc: 0.6080 - val_loss: 0.6699 - val_acc: 0.6250\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 14s - loss: 0.6715 - acc: 0.6150 - val_loss: 0.6602 - val_acc: 0.6250\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 14s - loss: 0.6711 - acc: 0.6150 - val_loss: 0.6652 - val_acc: 0.6250\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 14s - loss: 0.6705 - acc: 0.6150 - val_loss: 0.6589 - val_acc: 0.6250\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 14s - loss: 0.6654 - acc: 0.6150 - val_loss: 0.6621 - val_acc: 0.6250\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 14s - loss: 0.6551 - acc: 0.6125 - val_loss: 0.6501 - val_acc: 0.6250\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 14s - loss: 0.6195 - acc: 0.6620 - val_loss: 0.6503 - val_acc: 0.6000\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 14s - loss: 0.5471 - acc: 0.7250 - val_loss: 0.6364 - val_acc: 0.6500\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 14s - loss: 0.4417 - acc: 0.7990 - val_loss: 0.8824 - val_acc: 0.4375\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 14s - loss: 0.3093 - acc: 0.8710 - val_loss: 0.6941 - val_acc: 0.6563\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 14s - loss: 0.1239 - acc: 0.9600 - val_loss: 1.2869 - val_acc: 0.5688\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 14s - loss: 0.0499 - acc: 0.9845 - val_loss: 1.8554 - val_acc: 0.5813\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 14s - loss: 0.0271 - acc: 0.9915 - val_loss: 2.1757 - val_acc: 0.6313\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 14s - loss: 0.0244 - acc: 0.9915 - val_loss: 2.2243 - val_acc: 0.6000\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 14s - loss: 0.0135 - acc: 0.9945 - val_loss: 2.1059 - val_acc: 0.5875\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 14s - loss: 0.0030 - acc: 0.9995 - val_loss: 2.7546 - val_acc: 0.6188\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 14s - loss: 0.0092 - acc: 0.9985 - val_loss: 3.1135 - val_acc: 0.6188\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 14s - loss: 0.0059 - acc: 0.9980 - val_loss: 2.6729 - val_acc: 0.6438\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 14s - loss: 8.9102e-04 - acc: 1.0000 - val_loss: 3.0494 - val_acc: 0.6563\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 14s - loss: 0.0012 - acc: 0.9995 - val_loss: 3.1866 - val_acc: 0.6563\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 14s - loss: 0.0022 - acc: 0.9995 - val_loss: 2.8576 - val_acc: 0.5938\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 14s - loss: 2.2151e-04 - acc: 1.0000 - val_loss: 3.3524 - val_acc: 0.6188\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 14s - loss: 5.4964e-04 - acc: 1.0000 - val_loss: 3.3402 - val_acc: 0.6313\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 15s - loss: 0.0012 - acc: 0.9995 - val_loss: 3.5794 - val_acc: 0.6063\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 14s - loss: 6.4450e-05 - acc: 1.0000 - val_loss: 3.6496 - val_acc: 0.6188\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 14s - loss: 8.3163e-05 - acc: 1.0000 - val_loss: 3.5925 - val_acc: 0.6438\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 14s - loss: 1.6797e-05 - acc: 1.0000 - val_loss: 3.6088 - val_acc: 0.6438\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 14s - loss: 8.6199e-06 - acc: 1.0000 - val_loss: 3.7689 - val_acc: 0.6500\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 14s - loss: 6.8803e-05 - acc: 1.0000 - val_loss: 3.9716 - val_acc: 0.6375\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 14s - loss: 5.7752e-05 - acc: 1.0000 - val_loss: 3.9351 - val_acc: 0.6375\n"
     ]
    }
   ],
   "source": [
    "classifier = LeNetMnistClassifier(epochs=30, optimizer=keras.optimizers.Adadelta())\n",
    "classifier.preprocess_and_train(X_train=X_train0, y_train=y_train0, X_test=X_test0, y_test=y_test0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 13154147634325830913\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 104136704\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 18100578659675994095\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0\"\n",
      "]\n",
      "1.2.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare data set\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
